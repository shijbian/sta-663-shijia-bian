{
 "metadata": {
  "name": "",
  "signature": "sha256:fcd3e5fa8706b9548d6f0be0c39f246d5bcba88188171174a7e0ef2f575d1328"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "import scipy.linalg as la\n",
      "import scipy.stats as st"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as la\n",
      "import scipy.stats as st"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cPickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Latent Semantic Analysis (LSA) is a method for reducing the dimnesionality of documents treated as a bag of words. It is used for document classification, clustering and retrieval. For example, LSA can be used to search for prior art given a new patent application. In this homework, we will implement a small library for simple latent semantic analysis as a practical example of the application of SVD. The ideas are very similar to PCA.\n",
      "\n",
      "We will implement a toy example of LSA to get familiar with the ideas. If you want to use LSA or similar methods for statiscal language analyis, the most efficient Python library is probably [gensim](https://radimrehurek.com/gensim/) - this also provides an online algorithm - i.e. the training information can be continuously updated. Other useful functions for processing natural language can be found in the [Natural Lnaguage Toolkit](http://www.nltk.org/)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Note**: The SVD from scipy.linalg performs a full decomposition, which is inefficient since we only need to decompose until we get the first k singluar values. If the SVD from `scipy.linalg` is too slow, please use the `sparsesvd` function from the [sparsesvd](https://pypi.python.org/pypi/sparsesvd/) package to perform SVD instead.  You can install in the usual way with \n",
      "```\n",
      "!pip install sparsesvd\n",
      "```\n",
      "\n",
      "Then import the following\n",
      "```python\n",
      "from sparsesvd import sparsesvd \n",
      "from scipy.sparse import csc_matrix \n",
      "```\n",
      "\n",
      "and use as follows\n",
      "```python\n",
      "sparsesvd(csc_matrix(M), k=10)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 1 (10 points)**.  Calculating pairwise distance matrices.\n",
      "\n",
      "Suppose we want to construct a distance matrix between the rows of a matrix. For example, given the matrix \n",
      "\n",
      "```python\n",
      "M = np.array([[1,2,3],[4,5,6]])\n",
      "```\n",
      "\n",
      "the distance matrix using Euclidean distance as the measure would be\n",
      "```python\n",
      "[[ 0.000  1.414  2.828]\n",
      " [ 1.414  0.000  1.414]\n",
      " [ 2.828  1.414  0.000]] \n",
      "```\n",
      "if $M$ was a collection of column vectors.\n",
      "\n",
      "Write a function to calculate the pairwise-distance matrix given the matrix $M$ and some arbitrary distance function. Your functions should have the following signature:\n",
      "```\n",
      "def func_name(M, distance_func):\n",
      "    pass\n",
      "```\n",
      "\n",
      "0. Write a distance function for the Euclidean, squared Euclidean and cosine measures.\n",
      "1. Write the function using looping for M as a collection of row vectors.\n",
      "2. Write the function using looping for M as a collection of column vectors.\n",
      "3. Wrtie the function using broadcasting for M as a colleciton of row vectors.\n",
      "4. Write the function using broadcasting for M as a colleciton of column vectors. \n",
      "\n",
      "For 3 and 4, try to avoid using transposition (but if you get stuck, there will be no penalty for using transpoition). Check that all four functions give the same result when applied to the given matrix $M$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "import scipy.linalg as la\n",
      "import scipy.stats as st\n",
      "\n",
      "# Q1: Write a distance function for the Euclidean, squared Euclidean and cosine measures.\n",
      "M = np.array([[1,2,3],[4,5,6]])\n",
      "\n",
      "# Euclidean Distance\n",
      "# Pre: take the two vectors\n",
      "# Post: return the Euclidean distance between the two vectors\n",
      "def Euclidean(M1, M2):\n",
      "    store = np.sum((M1 - M2)**2,axis = 0)**0.5\n",
      "    return store\n",
      "\n",
      "# Square Euclidean Distance\n",
      "# Pre: take the two vectors\n",
      "# Post: return the Square Euclidean distance between the two vectors\n",
      "def EuclideanSq(M1, M2):\n",
      "    store = np.sum((M1 - M2)**2,axis = 0)\n",
      "    return store\n",
      "\n",
      "# cosine Distance\n",
      "# Pre: take the two vectors\n",
      "# Post: return the cosine measures between the two vectors\n",
      "def cos(M1, M2):\n",
      "    #store = np.sum(M1*M2)/((np.sum(M1**2)**0.5)*(np.sum(M2**2)**0.5))\n",
      "    left = np.sum(M1**2, axis=0)\n",
      "    right = np.sum(M2**2, axis=0)\n",
      "    store = np.sum(M1*M2, axis=0)/((left*right)**0.5)\n",
      "    return store\n",
      "\n",
      "\n",
      "# Q2: Write the function using looping for M as a collection of row vectors.\n",
      "# Pre: Take row vectors and the distance function\n",
      "# Post: the distance matrix calculated by the certain distance function\n",
      "def func_row(M, distance_func):\n",
      "    dim = M.shape\n",
      "    # make the matrix that can store the distance, the dimension is the row number of M\n",
      "    store = np.zeros(shape=(dim[0],dim[0]))\n",
      "    for i in range(dim[0]):\n",
      "        for j in range(dim[0]):\n",
      "            store[i,j] = distance_func(M[i,:], M[j,:])\n",
      "    return store\n",
      "\n",
      "# Q3\uff1aWrite the function using looping for M as a collection of colomn vectors.\n",
      "# Pre: Take colomn vectors and the distance function\n",
      "# Post: the distance matrix calculated by the certain distance function\n",
      "def func_col(M, distance_func):\n",
      "    dim = M.shape\n",
      "    # make the matrix that can store the distance, the dimension is the colomn number of M\n",
      "    store = np.zeros(shape=(dim[1],dim[1]))\n",
      "    for i in range(dim[1]):\n",
      "        for j in range(dim[1]):\n",
      "            store[i,j] = distance_func(M[:,i], M[:,j])\n",
      "    return store\n",
      "\n",
      "\n",
      "# Q4\uff1a Wrtie the function using broadcasting for M as a colleciton of row vectors.\n",
      "# Pre: Take row vectors and the distance function\n",
      "# Post: output the distance by using the pointed distance function\n",
      "def broad_col(M, distance_func):\n",
      "    # zip M to construct broadcasting\n",
      "    newM = zip(*M)\n",
      "    # M is the base point for broadcasting\n",
      "    store = np.array([distance_func(np.array(x)[:,None],M) for x in newM])\n",
      "    return store\n",
      "\n",
      "# Q5\uff1a function using broadcasting for M as a collection of column vectors\n",
      "# Pre: Take colomn vectors and the distance function\n",
      "# Post: output the distance by using the pointed distance function\n",
      "def broad_row(M, distance_func):\n",
      "    # zip M to construct broadcasting\n",
      "    newM = zip(*M.T)\n",
      "    # M is the base point for broadcasting\n",
      "    store = np.array([distance_func(np.array(x)[:,None],M.T) for x in newM])\n",
      "    return store\n",
      "\n",
      "print \"\\n\"\n",
      "print \"Broadcasting Euclidean Distance for the row matrix: \"\n",
      "print broad_row(M, Euclidean)\n",
      "print \"\\n\"\n",
      "print \"Euclidean Square Distance for the row matrix: \"\n",
      "print broad_row(M, EuclideanSq)\n",
      "print \"\\n\"\n",
      "print \"Cosine Distance for the row matrix: \"\n",
      "print broad_row(M, cos)\n",
      "print \"\\n\"\n",
      "print \"Broadcasting Euclidean Distance for the colomn matrix: \"\n",
      "print broad_col(M, Euclidean)\n",
      "print \"\\n\"\n",
      "print \"Euclidean Square Distance for the colomn matrix: \"\n",
      "print broad_col(M, EuclideanSq)\n",
      "print \"\\n\"\n",
      "print \"Cosine Distance for the colomn matrix: \"\n",
      "print broad_col(M, cos)\n",
      "\n",
      "print \"\\n\"\n",
      "print \"Euclidean Distance for the row matrix: \"\n",
      "print func_row(M, Euclidean)\n",
      "print \"\\n\"\n",
      "print \"Euclidean Square Distance for the row matrix: \"\n",
      "print func_row(M, EuclideanSq)\n",
      "print \"\\n\"\n",
      "print \"Cosine Distance for the row matrix: \"\n",
      "print func_row(M, cos)\n",
      "print \"\\n\"\n",
      "print \"Euclidean Distance for the colomn matrix: \"\n",
      "print func_col(M, Euclidean)\n",
      "print \"\\n\"\n",
      "print \"Euclidean Square Distance for the colomn matrix: \"\n",
      "print func_col(M, EuclideanSq)\n",
      "print \"\\n\"\n",
      "print \"Cosine Distance for the colomn matrix: \"\n",
      "print func_col(M, cos)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Broadcasting Euclidean Distance for the row matrix: \n",
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "\n",
        "\n",
        "Euclidean Square Distance for the row matrix: \n",
        "[[ 0 27]\n",
        " [27  0]]\n",
        "\n",
        "\n",
        "Cosine Distance for the row matrix: \n",
        "[[ 1.000  0.975]\n",
        " [ 0.975  1.000]]\n",
        "\n",
        "\n",
        "Broadcasting Euclidean Distance for the colomn matrix: \n",
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "\n",
        "\n",
        "Euclidean Square Distance for the colomn matrix: \n",
        "[[0 2 8]\n",
        " [2 0 2]\n",
        " [8 2 0]]\n",
        "\n",
        "\n",
        "Cosine Distance for the colomn matrix: \n",
        "[[ 1.000  0.991  0.976]\n",
        " [ 0.991  1.000  0.997]\n",
        " [ 0.976  0.997  1.000]]\n",
        "\n",
        "\n",
        "Euclidean Distance for the row matrix: \n",
        "[[ 0.000  5.196]\n",
        " [ 5.196  0.000]]\n",
        "\n",
        "\n",
        "Euclidean Square Distance for the row matrix: \n",
        "[[ 0.000  27.000]\n",
        " [ 27.000  0.000]]\n",
        "\n",
        "\n",
        "Cosine Distance for the row matrix: \n",
        "[[ 1.000  0.975]\n",
        " [ 0.975  1.000]]\n",
        "\n",
        "\n",
        "Euclidean Distance for the colomn matrix: \n",
        "[[ 0.000  1.414  2.828]\n",
        " [ 1.414  0.000  1.414]\n",
        " [ 2.828  1.414  0.000]]\n",
        "\n",
        "\n",
        "Euclidean Square Distance for the colomn matrix: \n",
        "[[ 0.000  2.000  8.000]\n",
        " [ 2.000  0.000  2.000]\n",
        " [ 8.000  2.000  0.000]]\n",
        "\n",
        "\n",
        "Cosine Distance for the colomn matrix: \n",
        "[[ 1.000  0.991  0.976]\n",
        " [ 0.991  1.000  0.997]\n",
        " [ 0.976  0.997  1.000]]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 2 (10 points)**. Write 3 functions to calculate the term frequency (tf), the inverse document frequency (idf) and the product (tf-idf). Each function should take a single argument `docs`, which is a dictionary of (key=identifier, value=dcoument text) pairs, and return an appropriately sized array. Convert '-' to ' ' (space), remove punctuation, convert text to lowercase and split on whitespace to generate a collection of terms from the dcoument text.\n",
      "\n",
      "- tf = the number of occurrences of term $i$ in document $j$\n",
      "- idf = $\\log \\frac{n}{1 + \\text{df}_i}$ where $n$ is the total number of documents and $\\text{df}_i$ is the number of documents in which term $i$ occurs.\n",
      "\n",
      "Print the table of tf-idf values for the following document collection\n",
      "\n",
      "```\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "```\n",
      "\n",
      "Note: You can use either a numpy array or pandas dataframe to store the matrix. However, we suggest using a Pnadas dataframe since that will allow you to keep track of the row (term) and column (document) names in a single object. Of course, you could also maintain a numpy matrix, a list of terms, and a list of documents separately if you prefer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from collections import Counter\n",
      "from pandas import *\n",
      "import pandas as pd\n",
      "from sklearn.metrics import confusion_matrix\n",
      "import string\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "import scipy.linalg as la\n",
      "import scipy.stats as st\n",
      "\n",
      "\n",
      "s1 = \"The quick brown fox\"\n",
      "s2 = \"Brown fox jumps over the jumps jumps jumps\"\n",
      "s3 = \"The the the lazy dog elephant.\"\n",
      "s4 = \"The the the the the dog peacock lion tiger elephant\"\n",
      "docs = {'s1': s1, 's2': s2, 's3': s3, 's4': s4}\n",
      "\n",
      "#Step 1: function tf:\n",
      "# Pre: pass one single document (a string/sentence) to function tf\n",
      "# Post: return the number of times each term occurs in this one single dcoument\n",
      "def tf(singleDoc):\n",
      "    lowText= singleDoc.replace(\"-\",' ')\n",
      "    # remove the punctuation within or outside of the word\n",
      "    for c in string.punctuation:\n",
      "        lowText = lowText.replace(c,'')\n",
      "    # convert the sentence to lowercase.\n",
      "    lowText = lowText.lower()\n",
      "    # Split the prepared lower case word by space\n",
      "    lowText = lowText.split()\n",
      "    count = Counter(lowText)\n",
      "    return count\n",
      "\n",
      "# Step 2: function tfs\n",
      "# Pre: pass the whole dictionary into the function\n",
      "# Post: Create a term freqeuncy dataframe from a dictionary of documents.\n",
      "def tfs (doc):\n",
      "    # first k is the key from the result of tf\n",
      "    # second key is the key from the big dictionary, doc in this function\n",
      "    df = pd.DataFrame({ k: tf(v) for k, v in doc.iteritems()}).fillna(0)\n",
      "    return df\n",
      "\n",
      "'''\n",
      "def idf(docs):\n",
      "    df = tfs(docs)\n",
      "    nrow = len(df.index)\n",
      "    ncol = len(df.columns)\n",
      "    idfarray = np.array([])\n",
      "    for i in range(nrow):\n",
      "        idfarray = np.append(idfarray, np.log(1.0*ncol/(1+sum(df.iloc[i,:]))))\n",
      "    idf = pd.DataFrame(idfarray, index=df.index, columns = ['document'])\n",
      "    return idf\n",
      "'''\n",
      "# Step 3: function idf\n",
      "# Pre: pass the whole documents as one parameter\n",
      "# Post: return the #of unique words * 1 matrixes with the inverse document frequecny\n",
      "def idf(docs):\n",
      "    df = tfs(docs)\n",
      "    nrow = len(df.index)\n",
      "    ncol = len(df.columns)\n",
      "    # create an arraye for storing inverse document frequecny\n",
      "    idfarray = np.array([])\n",
      "    for i in range(nrow):\n",
      "        #                                                 count the # of non zero in each row\n",
      "        idfarray = np.append(idfarray, np.log(1.0*ncol/(1+np.count_nonzero(df.iloc[i,:]!=0))))\n",
      "    # convert inverse document frequecny to be a data frame\n",
      "    idf = pd.DataFrame(idfarray, index=df.index, columns = ['document'])\n",
      "    return idf\n",
      "\n",
      "# Step 4: function tf-idf\n",
      "# Pre: pass the whole documents as one parameter\n",
      "# Post: return the #of unique words * num of doc matrixes with the inverse document frequecny * \n",
      "#       frequency of each word\n",
      "def tf_idf(docs):\n",
      "    df = tfs(docs)\n",
      "    nrow = len(df.index)\n",
      "    ncol = len(df.columns)\n",
      "    idfmatrix = idf(docs)\n",
      "    tfsmatrix = tfs(docs)\n",
      "    # Pointwise multiplication of idf and tfs\n",
      "    newdf = pd.DataFrame(tfsmatrix.values * idfmatrix.values, columns=tfsmatrix.columns, index=tfsmatrix.index)\n",
      "    return newdf\n",
      "\n",
      "print \"This is the tfs of the document:\"\n",
      "print tfs(docs)\n",
      "print \"\\n\"\n",
      "print \"This is the idf of the document:\"\n",
      "print idf(docs)\n",
      "print \"\\n\"\n",
      "print \"This is the tf idf of the document:\"\n",
      "print tf_idf(docs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is the tfs of the document:\n",
        "          s1  s2  s3  s4\n",
        "brown      1   1   0   0\n",
        "dog        0   0   1   1\n",
        "elephant   0   0   1   1\n",
        "fox        1   1   0   0\n",
        "jumps      0   4   0   0\n",
        "lazy       0   0   1   0\n",
        "lion       0   0   0   1\n",
        "over       0   1   0   0\n",
        "peacock    0   0   0   1\n",
        "quick      1   0   0   0\n",
        "the        1   1   3   5\n",
        "tiger      0   0   0   1\n",
        "\n",
        "\n",
        "This is the idf of the document:\n",
        "          document\n",
        "brown     0.287682\n",
        "dog       0.287682\n",
        "elephant  0.287682\n",
        "fox       0.287682\n",
        "jumps     0.693147\n",
        "lazy      0.693147\n",
        "lion      0.693147\n",
        "over      0.693147\n",
        "peacock   0.693147\n",
        "quick     0.693147\n",
        "the      -0.223144\n",
        "tiger     0.693147\n",
        "\n",
        "\n",
        "This is the tf idf of the document:\n",
        "                s1        s2        s3        s4\n",
        "brown     0.287682  0.287682  0.000000  0.000000\n",
        "dog       0.000000  0.000000  0.287682  0.287682\n",
        "elephant  0.000000  0.000000  0.287682  0.287682\n",
        "fox       0.287682  0.287682  0.000000  0.000000\n",
        "jumps     0.000000  2.772589  0.000000  0.000000\n",
        "lazy      0.000000  0.000000  0.693147  0.000000\n",
        "lion      0.000000  0.000000  0.000000  0.693147\n",
        "over      0.000000  0.693147  0.000000  0.000000\n",
        "peacock   0.000000  0.000000  0.000000  0.693147\n",
        "quick     0.693147  0.000000  0.000000  0.000000\n",
        "the      -0.223144 -0.223144 -0.669431 -1.115718\n",
        "tiger     0.000000  0.000000  0.000000  0.693147\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 3 (10 points)**. \n",
      "\n",
      "1. Write a function that takes a matrix $M$ and an integer $k$ as arguments, and reconstructs a reduced matrix using only the $k$ largest singular values. Use the `scipy.linagl.svd` function to perform the decomposition. This is the least squares approximation to the matrix $M$ in $k$ dimensions.\n",
      "\n",
      "2. Apply the function you just wrote to the following term-frequency matrix for a set of $9$ documents using $k=2$ and print the reconstructed matrix $M'$.\n",
      "```\n",
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "       [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "```\n",
      "\n",
      "3. Calculate the pairwise correlation matrix for the original matrix M and the reconstructed matrix using $k=2$ singular values (you may use [scipy.stats.spearmanr](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html) to do the calculations). Consider the fist 5 sets of documents as one group $G1$ and the last 4 as another group $G2$ (i.e. first 5 and last 4 columns). What is the average within group correlation for $G1$, $G2$ and the average cross-group correlation for G1-G2 using either $M$ or $M'$. (Do not include self-correlation in the within-group calculations.)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as la\n",
      "import scipy.stats as st\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "import scipy.linalg as la\n",
      "import scipy.stats as st\n",
      "\n",
      "# This is the matrix we are required for using and testing\n",
      "M = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
      "    [1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "    [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 1, 0, 0, 0, 0],\n",
      "    [0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
      "    [0, 1, 0, 0, 0, 0, 0, 0, 1],\n",
      "    [0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "    [0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "    [0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
      "\n",
      "# Question 1:\n",
      "# Pre: pass the two parameters: M is the matrix, k denotes the k largest singular values\n",
      "#      assumption k <= length of s\n",
      "# Post: reduced matrix constrcted by the k largest singular values\n",
      "def svdreduce(M,k):\n",
      "    U, s, Vh = la.svd(M, full_matrices=False)\n",
      "    #print U.shape, s.shape, Vh.shape\n",
      "    S = np.diag(s)\n",
      "    # make the rest of the diagnal len(s)-k be 0\n",
      "    for i in range(k,len(s)):\n",
      "        S[i,i]=0\n",
      "    #print S\n",
      "    Mreduced = np.dot(U, np.dot(S, Vh))\n",
      "    return Mreduced\n",
      "\n",
      "# Question 2:\n",
      "# Pre: pass parameter: matrix = M, k=2 \n",
      "# Post: print the reconstructed matrix M: Mreduced\n",
      "Mreduced = svdreduce(M,2)\n",
      "print \"This is the reconstructed reduced matrix k =2\"\n",
      "print Mreduced\n",
      "print \"\\n\"\n",
      "\n",
      "# Question 3:  \n",
      "\n",
      "# Part One: \n",
      "# Calculate the pairwise correlation matrix for the original \n",
      "# matrix M\n",
      "spearM = st.spearmanr(M)\n",
      "print \"The pairwise correlation matrix for the original matrix M\"\n",
      "print spearM[0]\n",
      "print \"\\n\"\n",
      "\n",
      "# Part Two: \n",
      "# Calculate the pairwise correlation matrix for the reconstructed \n",
      "# matrix M using k=2 singular value: Mreduced\n",
      "spearMreduced = st.spearmanr(Mreduced)\n",
      "print \"The pairwise correlation matrix for the reconstructed matrix M\"\n",
      "print spearMreduced[0]\n",
      "print \"\\n\"\n",
      "\n",
      "# Consider the fist 5 sets of documents as one group G1 and the last 4 \n",
      "# as another group G2 (i.e. first 5 and last 4 columns). What is the average \n",
      "# within group correlation for G1, G2 and the average cross-group correlation \n",
      "# for G1-G2 using either M or M\u2032. (Do not include self-correlation in the within-group calculations.).\n",
      "\n",
      "# Part Three:\n",
      "# Pre: Consider group G1\uff1athe fist 5 sets of documents \n",
      "#      parameter passed is pairwise correlation matrix \n",
      "# Post: the average within group correlation for G1\n",
      "def withincorG1(M):\n",
      "    sum = 0\n",
      "    # the upper left 5 rows and 5 columns\n",
      "    for i in range(5):\n",
      "        for j in range(5):\n",
      "            if j != i:\n",
      "                sum += M[0][i,j]\n",
      "    # divided by 20 because 5*4=20, due to symmetrix matrix\n",
      "    print sum/20\n",
      "\n",
      "print \"Compute the average within group correlation for G1 for M\"\n",
      "withincorG1(spearM)\n",
      "print \"\\n\"\n",
      "print \"Compute the average within group correlation for G1 for M prime \"\n",
      "withincorG1(spearMreduced)\n",
      "print \"\\n\"\n",
      "\n",
      "# Part Four:\n",
      "# Pre: parameter passed is pairwise correlation matrix \n",
      "# Post: the average within group correlation for G2\n",
      "def withincorG2(M):\n",
      "    sum = 0\n",
      "    # the right lower 4*4\n",
      "    for i in range(5,9):\n",
      "        for j in range(5,9):\n",
      "            if j != i:\n",
      "                sum += M[0][i,j]\n",
      "    # 12 cells so divided by 12, due to symmetrix matrix\n",
      "    print sum/12\n",
      "\n",
      "print \"Compute the average within group correlation for G2 for M\"\n",
      "withincorG2(spearM)\n",
      "print \"\\n\"\n",
      "print \"Compute the average within group correlation for G2 for M'\"\n",
      "withincorG2(spearMreduced)\n",
      "print \"\\n\"\n",
      "\n",
      "# Part Five:\n",
      "# Pre: parameter passed is pairwise correlation matrix \n",
      "# Post: the average cross group correlation\n",
      "def crossgroup(M):\n",
      "    sum = 0\n",
      "    # the upper right 5*4\n",
      "    for i in range(5):\n",
      "        for j in range(5,9):\n",
      "            sum += M[0][i,j]\n",
      "    # divided by 20 because 20 cells, due to symmetrix matrix\n",
      "    print sum/20\n",
      "\n",
      "print \"Compute the average cross group correlation for M\"\n",
      "crossgroup(spearM)\n",
      "print \"\\n\"\n",
      "print \"Compute the average cross group correlation for for M' \"\n",
      "crossgroup(spearMreduced)    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is the reconstructed reduced matrix k =2\n",
        "[[ 0.162  0.400  0.379  0.468  0.176 -0.053 -0.115 -0.159 -0.092]\n",
        " [ 0.141  0.370  0.329  0.400  0.165 -0.033 -0.071 -0.097 -0.043]\n",
        " [ 0.152  0.505  0.358  0.410  0.236  0.024  0.060  0.087  0.124]\n",
        " [ 0.258  0.841  0.606  0.697  0.392  0.033  0.083  0.122  0.187]\n",
        " [ 0.449  1.234  1.051  1.266  0.556 -0.074 -0.155 -0.210 -0.049]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.160  0.582  0.375  0.417  0.277  0.056  0.132  0.189  0.217]\n",
        " [ 0.218  0.550  0.511  0.628  0.243 -0.065 -0.143 -0.197 -0.108]\n",
        " [ 0.097  0.532  0.230  0.212  0.267  0.137  0.315  0.444  0.425]\n",
        " [-0.061  0.232 -0.139 -0.266  0.145  0.240  0.546  0.767  0.664]\n",
        " [-0.065  0.335 -0.146 -0.301  0.203  0.306  0.695  0.977  0.849]\n",
        " [-0.043  0.254 -0.097 -0.208  0.152  0.221  0.503  0.707  0.616]]\n",
        "\n",
        "\n",
        "The pairwise correlation matrix for the original matrix M\n",
        "[[ 1.000 -0.192  0.000  0.073 -0.333 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.192  1.000  0.000 -0.127  0.577 -0.302 -0.447 -0.577 -0.192]\n",
        " [ 0.000  0.000  1.000  0.438  0.000 -0.213 -0.316 -0.408 -0.408]\n",
        " [ 0.073 -0.127  0.438  1.000 -0.330 -0.172 -0.256 -0.330 -0.330]\n",
        " [-0.333  0.577  0.000 -0.330  1.000 -0.174 -0.258 -0.333 -0.333]\n",
        " [-0.174 -0.302 -0.213 -0.172 -0.174  1.000  0.674  0.522 -0.174]\n",
        " [-0.258 -0.447 -0.316 -0.256 -0.258  0.674  1.000  0.775  0.258]\n",
        " [-0.333 -0.577 -0.408 -0.330 -0.333  0.522  0.775  1.000  0.556]\n",
        " [-0.333 -0.192 -0.408 -0.330 -0.333 -0.174  0.258  0.556  1.000]]\n",
        "\n",
        "\n",
        "The pairwise correlation matrix for the reconstructed matrix M\n",
        "[[ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.846  1.000  0.846  0.846  0.972 -0.557 -0.557 -0.557 -0.480]\n",
        " [ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 1.000  0.846  1.000  1.000  0.719 -0.837 -0.837 -0.837 -0.802]\n",
        " [ 0.719  0.972  0.719  0.719  1.000 -0.389 -0.389 -0.389 -0.298]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.837 -0.557 -0.837 -0.837 -0.389  1.000  1.000  1.000  0.979]\n",
        " [-0.802 -0.480 -0.802 -0.802 -0.298  0.979  0.979  0.979  1.000]]\n",
        "\n",
        "\n",
        "Compute the average within group correlation for G1 for M\n",
        "0.0105776866299\n",
        "\n",
        "\n",
        "Compute the average within group correlation for G1 for M prime \n",
        "0.866666666667\n",
        "\n",
        "\n",
        "Compute the average within group correlation for G2 for M\n",
        "0.43511771482\n",
        "\n",
        "\n",
        "Compute the average within group correlation for G2 for M'\n",
        "0.98951048951\n",
        "\n",
        "\n",
        "Compute the average cross group correlation for M\n",
        "-0.307562188906\n",
        "\n",
        "\n",
        "Compute the average cross group correlation for for M' \n",
        "-0.677759358117\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise 4 (20 points)**. Clustering with LSA\n",
      "\n",
      "1. Begin by loading a pubmed database of selected article titles using 'cPickle'. With the following:\n",
      "```import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))```\n",
      "\n",
      "    Create a tf-idf matrix for every term that appears at least once in any of the documents. What is the shape of the tf-idf matrix? \n",
      "\n",
      "2. Perform SVD on the tf-idf matrix to obtain $U \\Sigma V^T$ (often written as $T \\Sigma D^T$ in this context with $T$ representing the terms and $D$ representing the documents). If we set all but the top $k$ singular values to 0, the reconstructed matrix is essentially $U_k \\Sigma_k V_k^T$, where $U_k$ is $m \\times k$, $\\Sigma_k$ is $k \\times k$ and $V_k^T$ is $k \\times n$. Terms in this reduced space are represented by $U_k \\Sigma_k$ and documents by $\\Sigma_k V^T_k$. Reconstruct the matrix using the first $k=10$ singular values.\n",
      "\n",
      "3. Use agglomerative hierachical clustering with complete linkage to plot a dendrogram and comment on the likely number of  document clusters with $k = 100$. Use the dendrogram function from [SciPy ](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.cluster.hierarchy.dendrogram.html).\n",
      "\n",
      "4. Determine how similar each of the original documents is to the new document `mystery.txt`. Since $A = U \\Sigma V^T$, we also have $V = A^T U S^{-1}$ using orthogonality and the rule for transposing matrix products. This suggests that in order to map the new document to the same concept space, first find the tf-idf vector $v$ for the new document - this must contain all (and only) the terms present in the existing tf-idx matrix. Then the query vector $q$ is given by $v^T U_k \\Sigma_k^{-1}$. Find the 10 documents most similar to the new document and the 10 most dissimilar. \n",
      "\n",
      "5. Many documents often have some boilerplate material such as organization information, Copyright, etc. at the front or back of the document. Does it matter that the front and back matter of each document is essentially identical for either LSA-based clustering (part 3) or information retrieval (part 4)? Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.cluster.hierarchy import linkage, dendrogram\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import glob\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "%matplotlib inline\n",
      "%precision 4\n",
      "plt.style.use('ggplot')\n",
      "\n",
      "import scipy.linalg as la\n",
      "import scipy.stats as st\n",
      "\n",
      "# Question 1:\n",
      "# loading a pubmed database \n",
      "# Begin by loading a pubmed database of selected article titles using 'cPickle'. With the following: import cPickle\n",
      "docs = cPickle.load(open('pubmed.pic'))\n",
      "\n",
      "# Create a tf-idf matrix for every term\n",
      "tfidf = tf_idf(docs)\n",
      "print \"The shape of the tf-idf matrix:\"\n",
      "print tfidf.shape\n",
      "print \"\\n\"\n",
      "\n",
      "# Question 2:\n",
      "def svdreduce(M,k):\n",
      "    # SVD on the tf-idf matrix \n",
      "    U, s, Vh = la.svd(M, full_matrices=False)\n",
      "    #print U.shape, s.shape, Vh.shape\n",
      "    S = np.diag(s)\n",
      "    # set all but the top k singular values to 0\n",
      "    for i in range(k,len(s)):\n",
      "        S[i,i]=0\n",
      "    # reconstructed matrix U(mk)*Sigma(kk)*V(kn)\n",
      "    Mreduced = np.dot(U, np.dot(S, Vh))\n",
      "    return Mreduced\n",
      "# Reconstruct the matrix using the first k=10 singular values\n",
      "tfidf_10 = svdreduce(tfidf, 10)\n",
      "print \"This is the reconstructed matrix using the first k=10 singular values:\"\n",
      "print tfidf_10\n",
      "print \"\\n\"\n",
      "\n",
      "# Question 3: Use agglomerative hierachical clustering with complete linkage to \n",
      "#         plot a dendrogram and comment on the likely number of document clusters with k=100 \n",
      "\n",
      "# Reconstruct the matrix using the first k=100 singular values\n",
      "tfidf_100 = svdreduce(tfidf, 100)\n",
      "#print \"This is the reconstructed matrix using the first k=100 singular values:\"\n",
      "#print tfidf_100\n",
      "#print \"\\n\"\n",
      "\n",
      "# choose the euclean as the distance type (Professor said it's ok to use distance type other than cosine)\n",
      "tfidf_100_distance = func_col(tfidf_100, Euclidean)\n",
      "#print \"This is the euclean distancefor the k=100 singular values:\"\n",
      "#print tfidf_100_distance\n",
      "#print \"\\n\"\n",
      "\n",
      "# Dendrogram for the complete linckage:\n",
      "linkage_matrix = linkage(tfidf_100_distance, method='complete')\n",
      "ddata = dendrogram(linkage_matrix, color_threshold=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The shape of the tf-idf matrix:\n",
        "(6488, 178)\n",
        "\n",
        "\n",
        "This is the reconstructed matrix using the first k=10 singular values:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.044 -0.051  0.177 ...,  0.023  0.127 -0.210]\n",
        " [ 0.003  0.043  0.008 ...,  0.035  0.029  0.092]\n",
        " [ 0.002 -0.076  0.066 ..., -0.041 -0.009 -0.180]\n",
        " ..., \n",
        " [ 0.008  0.009  0.016 ...,  0.006  0.026  0.012]\n",
        " [ 0.031  0.122  0.070 ...,  0.093  0.128  0.198]\n",
        " [ 0.008  0.016  0.017 ...,  0.011  0.031  0.026]]\n",
        "\n",
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b41a490>"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 4\n",
      "# Open new document mystery.txt\n",
      "text = open('mystery.txt').read()\n",
      "# Create the mystery as a new dictionary called myst\n",
      "myst = {'mystery':text}\n",
      "\n",
      "# create the tfidf for the myst\n",
      "tfidf2 = tf_idf(myst)\n",
      "\n",
      "# This is the tfidf for the docs\n",
      "tfidf = tf_idf(docs)\n",
      "\n",
      "# Left joint the tfidf for docs and the tfidf for myst: one more colomn added on 178 colomn\n",
      "# so the colomn becomes 179, but the number of row does not change\n",
      "# By assumption, the size of myst is always smaller than that of the docs\n",
      "newdocs = pd.DataFrame.join(tfidf,tfidf2,how='left').fillna(0)\n",
      "print \"This is the new dimension for the new combined documents: docs + mystery\"\n",
      "print newdocs.shape\n",
      "print \"\\n\"\n",
      "\n",
      "#print U, s, Vh\n",
      "U, s, Vh = la.svd(tfidf, full_matrices=False)\n",
      "print \"Shape of U, s, Vh are:\"\n",
      "print U.shape, s.shape, Vh.shape\n",
      "print \"\\n\"\n",
      "\n",
      "# I use k = 10, ignore the other part of the U, s, Vh matrix and reconstruct the matrix\n",
      "S = np.diag(s[:10])\n",
      "U_10=U[:,:10]\n",
      "Vh_10=Vh[:10,:]\n",
      "newV = np.dot(newdocs.T,np.dot(U_10,la.inv(S)))\n",
      "print \"This is the shape of the reconstructed matrix with k = 10: \"\n",
      "print newV.shape\n",
      "print \"\\n\"\n",
      "\n",
      "# I use euclean distance to compare the similarity\n",
      "# I calculate the distance for each of the first 178 rows between the 179th row\n",
      "# The 179th row represents the new mystery doc\n",
      "storeDis = []\n",
      "nrow = newV.shape[0]\n",
      "for i in range(nrow-1):\n",
      "    # store the distance in the storeDis, there will be 178 distance\n",
      "    storeDis.append(Euclidean(newV[i,:], newV[nrow-1,:]))\n",
      "\n",
      "# The distance is sorted by increasing order, sort stores the index \n",
      "sort = [i[0] for i in sorted(enumerate(storeDis), key=lambda x:x[1])]\n",
      "\n",
      "print \"This is the top 10 similar document index and the title: \"\n",
      "print sort[:10]\n",
      "print \"\\n\"\n",
      "print tfidf.columns[sort[:10]]\n",
      "print \"\\n\"\n",
      "print \"This is the least 10 similar (10 most different) document index and the title: \"\n",
      "print sort[nrow-10:]\n",
      "print \"\\n\"\n",
      "print tfidf.columns[sort[nrow-10:]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This is the new dimension for the new combined documents: docs + mystery\n",
        "(6488, 179)\n",
        "\n",
        "\n",
        "Shape of U, s, Vh are:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(6488, 178) (178,) (178, 178)\n",
        "\n",
        "\n",
        "This is the shape of the reconstructed matrix with k = 10: \n",
        "(179, 10)\n",
        "\n",
        "\n",
        "This is the top 10 similar document index and the title: \n",
        "[43, 130, 151, 133, 73, 15, 134, 18, 112, 63]\n",
        "\n",
        "\n",
        "Index([u'Correction: Angiopoietin-2 and Angiopoietin-2/Angiopoietin-1 Ratio as Indicators of Potential Severity of Plasmodium vivax Malaria in Patients with Thrombocytopenia.', u'Prevalence and Determinants of Anemia in Older People With Diabetes Attending an Outpatient Clinic: A Cross-Sectional Audit.', u'Synthesis and biological evaluation of febrifugine analogues.', u'Real-time deformability cytometry: on-the-fly cell mechanical phenotyping.', u'Gestational Diabetes Mellitus Screening Using the One-Step Versus Two-Step Method in a High-Risk Practice.', u'An efficient synthesis method targeted to marine alkaloids marinacarbolines A-D and their antitumor activities.', u'Relationship of light scatter change and cdc42-regulated actin status.', u'Antiprotozoal Activity and DNA Binding of Dicationic Acridones.', u'Myocarditis associated with Plasmodium vivax malaria: a case report.', u'Evaluation of Ebola virus Inactivation Procedures for Plasmodium falciparum Malaria Diagnostics.'], dtype='object')\n",
        "\n",
        "\n",
        "This is the least 10 similar (10 most different) document index and the title: \n",
        "[122, 135, 53, 170, 123, 153, 50, 21, 39]\n",
        "\n",
        "\n",
        "Index([u'PD-1/PD-L1 Costimulatory Pathway-induced Mouse Islet Transplantation Immune Tolerance.', u'Removal of peanut allergen Ara h 1 from common hospital surfaces, toys and books using standard cleaning methods.', u'Dopamine Increases CD14+CD16+ Monocyte Migration and Adhesion in the Context of Substance Abuse and HIV Neuropathogenesis.', u'Vitamin D deficiency in pregnant women impairs Regulatory T cell function.', u'Phenotypic profiling of CD8 + T cells during Plasmodium vivax blood-stage infection.', u'The antitumor compound triazoloacridinone C-1305 inhibits FLT3 kinase activity and potentiates apoptosis in mutant FLT3-ITD leukemia cells.', u'Dissociation between anti-porcine albumin and anti-Gal antibody responses in non-human primate recipients of intraportal porcine islet transplantation.', u'Asian sand dust aggregate causes atopic dermatitis-like symptoms in Nc/Nga mice.', u'Clinical practice guideline: allergic rhinitis.'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Question 5:\n",
      "# Many documents often have some boilerplate material such as organization information,\n",
      "# Copyright, etc. at the front or back of the document. Does it matter that the front \n",
      "# and back matter of each document is essentially identical for either LSA-based clustering \n",
      "# (part 3) or information retrieval (part 4)? Why or why not?\n",
      "\n",
      "print \"This does not matter. LSA-based clustering compares the relative distance/similarity between the documents. If \"\n",
      "print \"all the book has some parts similar, the LSA-based clustering only consider and compare the most disimmilar \"\n",
      "print \"part. The similar parts are just cancelled out. \""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "This does not matter. LSA-based clustering compares the relative distance/similarity between the documents. If \n",
        "all the book has some parts similar, the LSA-based clustering only consider and compare the most disimmilar \n",
        "part. The similar parts are just cancelled out. \n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    }
   ],
   "metadata": {}
  }
 ]
}